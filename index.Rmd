---
title: "Practical Machine Learning Project Report"
author: "Raman Chandrasekar"
date: "June 4, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background

This report describes the approach used to train and test on the [Weight Lifting Exercise Dataset](http://groupware.les.inf.puc-rio.br/har), which contains data acquired from subjects correctly and incorrectly doing barbell lifts.This report is part of the final project of the Coursera course on <i>Practical Machine Language Learning</i>.


## Overall approach
From previous experience with machine learning, I have become fond of using Random Forest (rf) for classification.

I initially ran rf on the training data as supplied, to get a baseline. I then refined the dataset to get rid of columns with a preponderance of NA and blank values etc, and ran a number of machine learning algorithms, including Stochastic Gradient Boosting (gbm),  Neural Network (nnet) and Random Forest (rf). Consistent with my earlier experience, I found Random Forest to be the best, and have therefore reported rf results.

When I tried out some approaches on the training and testing data, I found that pretty much any approach takes time on my 2010 MacBook Pro, even with 8GB of memory. I found the article [Improving Performance of Random Forest in caret::train()](https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md) by Len Greski very useful, primarily in the making the processing parallel.

As per directions, I am not including the R code I wrote to run the training and prediction.

## Data Pruning
On examining the raw data, of the 19,622 rows and 160 columns, it is clear that there are many columns with NA values and with blank cells. I used the options to read.csv to mark NAs and blanks as NA values in the dataframes read-in for training and testing. I then noticed the initial columns includes names of the subjects and timestamps, which should not be predictor variables. I therefore deleted the first six columns, namely, <i>id, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp and new_window</i>.

In columns with a number of NA values, I found that there tended to be very few non-NA values. I therefore decided to keep only columns with non-NA values, and delete all other columns. That left me with 53 columns of predictor columns, and the <i>classe</i> result (predicted) column. In further experiemnts, I got very high accuracy, but found a significant correlation between the <i>num_window</i> column and the 
<i>classe</i> column. In the next iteration, I deleted this <i>num_window</i> column as well, and re-ran random forest on 19,622 rows and 52 columns of predictor values.

I used the <i>cor</i> function to identify correlated columns, and could have ignored a few more columns, but chose not to do that.

## Running the model

As recommended in the Greski article, I set up my (4-core) laptop as a 3-core cluster (leaving 1 core for the OS). Because 10-fold cross-validation (CV) was taking far-too long to finish running, I set up traincontrol to do 5-fold cross validation, and set it up to use the cluster defined. I then loaded the training data into a dataframe, and trained the **classe** feature/variable against all other features/variables. When the training ended, I examined the model created, got relevant statistics and stopped the cluster. The results from the final run are presented in the rest of this report.

<pre>
Model details
 Random Forest 
 
 19622 samples
 52 predictor
 5 classes: 'A', 'B', 'C', 'D', 'E' 

 Resampling: Cross-Validated (5 fold) 
 Summary of sample sizes: 15697, 15698, 15697, 15698, 15698 
 Resampling results across tuning parameters:
         
         mtry  Accuracy   Kappa    
 2    0.9943940  0.9929083
 27    0.9944450  0.9929732
 52    0.9889919  0.9860739
 
 Accuracy was used to select the optimal model using  the largest value.
 The final value used for the model was mtry = 27.
 </pre>


The finalModel details are as follows:
<pre>

 Type of random forest: classification
 Number of trees: 500
 No. of variables tried at each split: 27
 
 OOB estimate of  error rate: 0.4%
 
 Confusion matrix:
      A    B    C    D    E class.error
 A 5574    4    1    0    1 0.001075269
 B   17 3776    4    0    0 0.005530682
 C    0   10 3402   10    0 0.005844535
 D    0    1   21 3192    2 0.007462687
 E    0    1    3    3 3600 0.001940671

</pre>

 Here is the confusion matrix on the training data, with 5-fold CV:

 <pre>
 Cross-Validated (5 fold) Confusion Matrix 
 
 (entries are percentual average cell counts across resamples)
 
 Reference
     Prediction    A    B    C    D    E
 A                28.4  0.1  0.0  0.0  0.0
 B                 0.0 19.2  0.1  0.0  0.0
 C                 0.0  0.0 17.3  0.2  0.0
 D                 0.0  0.0  0.0 16.2  0.0
 E                 0.0  0.0  0.0  0.0 18.3
 
 Accuracy (average) : 0.9944
 </pre>
 As can be seen, the out of sample accuracy of 0.9944 is extremely high, with a minor amount of confusion between actual C and predicted D classes. In fact, with the <i>num_window</i> column, the accuracy was even higher, namely: 0.9979.
 
The model was used to predict the <i>classe</i> for the test data, and per quiz results, was 100% accurate. A look at <i>type = "prob"</i> results in the prediction shows 1.0 or close to 1.0 probabilities for the correct class.

## Summary
This was an interesting open-ended non-trivial machine learning problem. As expected, some thinking about and playing with the data led to cleaning up the data and ignoring more than 100 data-sparse columns. Random forest turned out to be a good classifier technique, even if it took a while to run. The results from the model turned gave 100% accuracy on the test data, which was also satisfying. On the whole, this project was a very useful learning experience.

